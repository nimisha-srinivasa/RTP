//index_gen - takes in trec formatted xml (actually sgml), and outputs an index for each document
//output format: <feature> <docno> <value>
//there is a feature for each word with the frequency, and also for each pair of words with a dist <= 10 there is a minimum distance
#include "IndexGenerator.h"
#include "../../../utils/Serializer.h"
#include <set>

//#define DOPAIRS 1

static std::map<termid_t, std::vector<index_pair> > postings;
static std::map<termid_t, std::string> dictionary;
static std::set<std::string> bigrams;

//bool verbose = true;
bool verbose = false;
bool do_docs = false;

int generate_index(int input_fd, std::string fname, bool debug) {
    LineCorpusScanner *corpus = new LineCorpusScanner(input_fd);
    DocumentScanner *scanner = corpus->getScanner();
    uint64_t fpos;
    docid_t docid;
    std::string doc_name;
    std::vector<std::string> title_terms;
    std::vector<std::string> body_terms;
    std::set<termid_t> doc_title_terms,doc_body_terms;
    std::map<std::string,uint64_t> stats;
#ifdef DOPAIRS
    std::string wordbuffer[MAX_DIST];
#endif

    uint64_t ntitle_words = 0;
    uint64_t nbody_words = 0;
    uint64_t ntitle_terms = 0;
    uint64_t nbody_terms = 0;
    uint64_t ndocs = 0;

    serializer * docs_ser;
    if (do_docs) {
        docs_ser = new serializer(fname + ".docs", serializer::WRITE);
    } else {
        docs_ser = NULL;
    }

    while (scanner->nextDocument(fpos,docid,doc_name,title_terms,body_terms)) {
        doc_title_terms.clear();
        doc_body_terms.clear();
        ++ndocs;
        uint32_t pos = 0;
        for(std::vector<std::string>::const_iterator it = title_terms.begin(); it != title_terms.end(); ++it) {
            ++ntitle_words;
            std::string term = make_title_term(*it);
            termid_t tid = compute_term_id(term);
            postings[tid].push_back(index_pair(docid,pos));

            doc_title_terms.insert(tid);
            std::pair<std::map<termid_t, std::string>::iterator,bool> i = dictionary.insert(std::pair<termid_t,std::string>(tid,term));
            if (!i.second) { //element already there
                if (i.first->second != term) {
                    std::cerr << "hash collision between '" << term << "' and '" << i.first->second << "'\n";
                }
            } else {
                ++ntitle_terms;
            }

#ifdef DOPAIRS
            wordbuffer[pos % MAX_DIST] = *it;
            for(uint32_t pairdist = MAX_DIST-1; pairdist > 0; pairdist--) {
                if (pos > pairdist) {
                    std::string pair = make_title_pair(wordbuffer[(pos+MAX_DIST-pairdist)%MAX_DIST],wordbuffer[pos%MAX_DIST]);
                    bigrams.insert(pair);
                }
            }
#endif
            pos++;
        }
        pos = 0;
        for(std::vector<std::string>::const_iterator it = body_terms.begin(); it != body_terms.end(); ++it) {
            ++nbody_words;
            std::string term = *it;
            termid_t tid = compute_term_id(term);
            postings[tid].push_back(index_pair(docid,pos));

            doc_body_terms.insert(tid);
            std::pair<std::map<termid_t, std::string>::iterator,bool> i = dictionary.insert(std::pair<termid_t,std::string>(tid,term));
            if (!i.second) { //element already there
                if (i.first->second != term) {
                    std::cerr << "hash collision between '" << term << "' and '" << i.first->second << "'\n";
                }
            } else {
                ++nbody_terms;
            }

#ifdef DOPAIRS
            wordbuffer[pos % MAX_DIST] = term;
            for(uint32_t pairdist = MAX_DIST-1; pairdist > 0; pairdist--) {
                if (pos > pairdist) {
                    std::string pair = make_pair(wordbuffer[(pos+MAX_DIST-pairdist)%MAX_DIST],wordbuffer[pos%MAX_DIST]);
                    bigrams.insert(pair);
                }
            }
#endif
            pos++;
        }
        if (docs_ser) {
            docs_ser->write_uint64(docid);
            docs_ser->write_uint64(fpos);
            docs_ser->write_uint32(title_terms.size());
            docs_ser->write_uint32(body_terms.size());
            docs_ser->write_uint32(doc_title_terms.size());
            docs_ser->write_uint32(doc_body_terms.size());
        }
    }
    stats["ntitle_terms"] = ntitle_terms;
    stats["ntitle_words"] = ntitle_words;
    stats["nbody_terms"] = nbody_terms;
    stats["nbody_words"] = nbody_words;
    stats["nterms"] = ntitle_terms + nbody_terms;
    stats["nwords"] = ntitle_words + nbody_words;
    stats["ndocs"] = ndocs;

    //std::cerr << "\nfinished scan, dumping index...\n";
    //std::map<termid_t, std::vector<index_pair> >(postings).swap(postings); //resize posting capacity to actual size
    if (docs_ser) {
        docs_ser->flush();
        delete docs_ser;
    }
    return dump_index(fname,postings,dictionary,bigrams,stats);
}

int dump_index(std::string fname, std::map<termid_t, std::vector<index_pair> > &postings, std::map<termid_t, std::string> &dictionary, std::set<std::string> &bigrams,std::map<std::string,uint64_t> stats) {
    serializer * dict_ser = new serializer(fname + ".dct", serializer::WRITE);
    serializer * terms_ser = new serializer(fname + ".trm", serializer::WRITE);
    serializer * postings_ser = new serializer(fname + ".idx", serializer::WRITE);
    serializer * stats_ser = new serializer(fname + ".stats", serializer::WRITE);
#ifdef DOPAIRS
    serializer bigrams_ser = new serializer(fname + ".bgm", serializer::WRITE);
#endif

#ifdef DOPAIRS
    for(std::set<std::string>::const_iterator it = bigrams.begin(); it != bigrams.end(); ++it) {
        std::string b = *it + "\n";
        bigrams_ser.write_bytes(b.c_str(), b.size());
    }
#endif


    termid_t prev_tid = 0;

    std::map<docid_t, std::vector<uint32_t> > posting;
    for(std::map<termid_t, std::vector<index_pair> >::const_iterator it = postings.begin(); it != postings.end(); ++it) {
        //while(!postings.empty()) {
        //std::map<termid_t, std::vector<index_pair> >::iterator it = postings.begin();
        termid_t tid = it->first;
        std::string term = dictionary[tid];
        if (verbose) std::cout << term;
        //std::cout << " " << std::hex << tid << std::dec;

        posting.clear();
        //construct posting sorted by document
        for(std::vector<index_pair>::const_iterator pit = it->second.begin(); pit != it->second.end(); ++pit) {
            docid_t docid = pit->first;
            uint32_t pos = pit->second;
            posting[docid].push_back(pos);
        }
        stats["npostings"] += posting.size();


        //write to dictionary
        dict_ser->write_vbyte64(tid-prev_tid); //write delta termid
        dict_ser->write_bytes(term.c_str(),strlen(term.c_str())+1); //write term string with null terminator

        //write to term index
        terms_ser->write_uint64(tid); //write termid
        uint64_t pos = postings_ser->pos();
        terms_ser->write_uint64(pos); //write term position in postings file

        //now we write: 0) #docs 1) doc ids, 2) posting lengths, 3) posting positions
        postings_ser->write_vbyte32(posting.size()); //write #docs in posting

        docid_t prev_docid = 0;
        for(std::map<docid_t, std::vector<uint32_t> >::iterator it = posting.begin(); it != posting.end(); ++it) {
            docid_t docid = it->first;
            postings_ser->write_vbyte64(docid - prev_docid); //write delta docid
            prev_docid = docid + 1; //because docids are always 1+ apart
        }
        for(std::map<docid_t, std::vector<uint32_t> >::iterator it = posting.begin(); it != posting.end(); ++it) {
            uint32_t posting_length = it->second.size();
            postings_ser->write_s9(posting_length-1); //write posting length-1, because it can't be zero
        }

        //now iterate through posting by document
        if (verbose) {
            std::cout << " ndocs:" << posting.size();

            std::cout << " docs:";
            for(std::map<docid_t, std::vector<uint32_t> >::const_iterator it = posting.begin(); it != posting.end(); ++it) {
                std::cout << it->first << ",";
            }
            std::cout << " plens:";
            for(std::map<docid_t, std::vector<uint32_t> >::const_iterator it = posting.begin(); it != posting.end(); ++it) {
                std::cout << it->second.size() << ",";
            }
        }
        for(std::map<docid_t, std::vector<uint32_t> >::iterator it = posting.begin(); it != posting.end(); ++it) {
            std::sort(it->second.begin(), it->second.end()); //sort posting by pos
            //docid_t docid = it->first;
            if (verbose) {
                //std::cout << " " << docid << ":";
                std::cout << " p:";
            }
            uint32_t prev_pos = 0;
            for(std::vector<uint32_t>::const_iterator pit = it->second.begin(); pit != it->second.end(); ++pit) {
                if (verbose) {
                    std::cout << *pit << ",";
                }
                postings_ser->write_s9(*pit-prev_pos); //write posting postion
                prev_pos = *pit + 1; //because positions are always 1+ apart
            }
        }
        postings_ser->flush_s9();
        if (verbose) std::cout << "\n";
        prev_tid = tid + 1; //because tids are always 1+ apart
    }

    for(std::map<std::string,uint64_t>::const_iterator it = stats.begin(); it != stats.end(); ++it) {
        stats_ser->write_bytes(it->first.c_str(),strlen(it->first.c_str())+1); //write key string with null terminator
        stats_ser->write_vbyte64(it->second); //write delta termid
    }

    dict_ser->flush();
    terms_ser->flush();
    postings_ser->flush();
    stats_ser->flush();
#ifdef DOPAIRS
    bigrams_ser.flush();
    delete bigrams_ser;
#endif
    delete dict_ser;
    delete terms_ser;
    delete postings_ser;
    delete stats_ser;
    return 0;
}

int main(int argc, char *argv[]) {
    int input_fd;
    const char *index_path = "\0";

    int argi = 1;
    while(argi < argc) {
        const char *arg = argv[argi];
        argi++;
        if (0 == strcmp(arg,"--help")) {
        } else if (!strcmp(arg,"--index")) {
            if (argi >= argc) {
                std::cerr << "missing option to --index\n";
                return 1;
            }
            index_path = argv[argi++];
        } else if (0 == strcmp(arg,"--verbose") || 0 == strcmp(arg,"-v")) {
            verbose = true;
        } else if (0 == strcmp(arg,"--docs")) {
            do_docs = true;
        } else if (0 == strcmp(arg,"--quiet") || 0 == strcmp(arg,"-q")) {
            verbose = false;
        } else if (arg[0] == '-') {
            std::cerr << "unknown option \"" << arg << "\"\n";
            return -1;
        } else {
            argi--;
            break;
        }
    }

    if (argi < argc) {
        int fd = open(argv[argi++], O_RDONLY);
        if (fd < 0) {
            std::cerr << "failed to open corpus" << std::endl;
            exit(-1);
        }
        input_fd = fd;
    } else {
        input_fd = STDIN_FILENO;
    }

    if (index_path[0] == '\0' && argi < argc) index_path = argv[argi++];

    if (index_path[0] == '\0') {
        index_path = "index";
    }

    std::string fname(index_path);

    if (argi < argc) {
        std::cerr << "unknown extra options\n";
        return -1;
    }

    return generate_index(input_fd, fname, true);
}
